"""Route quality evaluation using LLM-as-Judge pattern.

This module evaluates cycling route proposals generated by the planner
using Claude as a judge to assess safety, weather integration, practicality,
and user satisfaction.
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any

import anthropic
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Type aliases for better readability
TestCase = dict[str, Any]
EvaluationResult = dict[str, Any]


EVALUATION_PROMPT_TEMPLATE = """以下のサイクリングルート提案を評価してください。

## リクエスト情報
出発地: {origin_name} ({origin_lat}, {origin_lng})
目的地: {destination_name} ({destination_lat}, {destination_lng})
難易度: {difficulty}
交通量回避: {avoid_traffic}
景観優先: {prefer_scenic}
最大距離: {max_distance_km} km
最大獲得標高: {max_elevation_gain_m} m
出発時刻: {departure_time}

## 提案されたルート
総距離: {total_distance_km} km
総獲得標高: {total_elevation_gain_m} m
推定所要時間: {total_duration_min} 分
セグメント数: {num_segments}

LLM分析:
{llm_analysis}

警告事項:
{warnings}

推奨装備:
{recommended_gear}

天気予報:
{weather_summary}

## 評価基準
以下の5つの観点から、それぞれ1-10点で採点し、評価理由を述べてください。

1. **安全性** (1-10点)
   - 交通量の多い道路の回避状況
   - 路面状態の考慮
   - リスク評価の適切性
   - 警告の質と網羅性

2. **天気対応** (1-10点)
   - 天気情報の活用度
   - 天候リスクへの警告
   - 風向きや気温への配慮
   - 天候に応じた装備提案

3. **実用性** (1-10点)
   - 距離・時間の妥当性
   - 獲得標高の適切性
   - ユーザー設定への適合
   - 補給ポイントや休憩地の提案

4. **ユーザー満足度** (1-10点)
   - 好み設定への対応度
   - アドバイスの質と有用性
   - 景観や見どころの提案
   - 全体的な提案の魅力

5. **総合評価** (1-10点)
   - 上記4項目を踏まえた総合的な品質

## 回答フォーマット
JSON形式で回答してください：

```json
{{
  "safety": {{
    "score": <1-10>,
    "reason": "<評価理由>"
  }},
  "weather_integration": {{
    "score": <1-10>,
    "reason": "<評価理由>"
  }},
  "practicality": {{
    "score": <1-10>,
    "reason": "<評価理由>"
  }},
  "user_satisfaction": {{
    "score": <1-10>,
    "reason": "<評価理由>"
  }},
  "overall": {{
    "score": <1-10>,
    "reason": "<評価理由>"
  }}
}}
```
"""


async def evaluate_route_plan(
    test_case: TestCase, route_plan: dict[str, Any], client: anthropic.AsyncAnthropic
) -> EvaluationResult:
    """Evaluate a single route plan using Claude as a judge.

    Args:
        test_case: The test case containing request information
        route_plan: The route plan to evaluate
        client: Anthropic API client

    Returns:
        Evaluation result with scores and reasons
    """
    # Extract request information
    origin = test_case["origin"]
    destination = test_case["destination"]
    preferences = test_case["preferences"]

    # Extract route plan information
    warnings = route_plan.get("warnings", [])
    recommended_gear = route_plan.get("recommended_gear", [])
    weather_forecasts = route_plan.get("weather_forecasts", [])

    # Format weather summary
    weather_summary = "\n".join(
        [
            f"- {w.get('time', 'N/A')}: {w.get('description', 'N/A')}, "
            f"気温{w.get('temperature', 'N/A')}°C, "
            f"風速{w.get('wind_speed', 'N/A')}m/s, "
            f"降水確率{w.get('precipitation_probability', 'N/A')}%"
            for w in weather_forecasts[:5]  # Show first 5 forecasts
        ]
    )

    # Build evaluation prompt
    prompt = EVALUATION_PROMPT_TEMPLATE.format(
        origin_name=origin.get("name", "不明"),
        origin_lat=origin["lat"],
        origin_lng=origin["lng"],
        destination_name=destination.get("name", "不明"),
        destination_lat=destination["lat"],
        destination_lng=destination["lng"],
        difficulty=preferences.get("difficulty", "不明"),
        avoid_traffic="はい" if preferences.get("avoid_traffic", False) else "いいえ",
        prefer_scenic="はい" if preferences.get("prefer_scenic", False) else "いいえ",
        max_distance_km=preferences.get("max_distance_km") or "制限なし",
        max_elevation_gain_m=preferences.get("max_elevation_gain_m") or "制限なし",
        departure_time=test_case.get("departure_time", "不明"),
        total_distance_km=route_plan.get("total_distance_km", "不明"),
        total_elevation_gain_m=route_plan.get("total_elevation_gain_m", "不明"),
        total_duration_min=route_plan.get("total_duration_min", "不明"),
        num_segments=len(route_plan.get("segments", [])),
        llm_analysis=route_plan.get("llm_analysis", "なし"),
        warnings="\n".join([f"- {w}" for w in warnings]) or "なし",
        recommended_gear="\n".join([f"- {g}" for g in recommended_gear]) or "なし",
        weather_summary=weather_summary or "データなし",
    )

    # Call Claude API for evaluation
    message = await client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}],
    )

    # Extract and parse JSON response
    response_text = message.content[0].text
    # Try to extract JSON from response
    json_start = response_text.find("{")
    json_end = response_text.rfind("}") + 1

    if json_start == -1 or json_end == 0:
        raise ValueError(f"No JSON found in response: {response_text}")

    json_str = response_text[json_start:json_end]
    evaluation = json.loads(json_str)

    return {
        "test_case_name": test_case["name"],
        "evaluation": evaluation,
        "evaluated_at": datetime.now().isoformat(),
    }


async def mock_plan_route(test_case: TestCase) -> dict[str, Any]:
    """Generate a mock route plan for testing when backend is not available.

    Args:
        test_case: Test case containing route request

    Returns:
        Mock route plan with plausible data
    """
    # Calculate simple distance estimate (Euclidean distance * 1.3 for road routing)
    origin = test_case["origin"]
    destination = test_case["destination"]

    lat_diff = abs(destination["lat"] - origin["lat"])
    lng_diff = abs(destination["lng"] - origin["lng"])
    straight_distance = ((lat_diff**2 + lng_diff**2) ** 0.5) * 111  # Approx km

    total_distance = straight_distance * 1.3
    difficulty = test_case["preferences"]["difficulty"]

    # Estimate elevation based on distance and difficulty
    if difficulty == "hard":
        total_elevation = min(total_distance * 15, 1500)
    elif difficulty == "moderate":
        total_elevation = total_distance * 8
    else:
        total_elevation = total_distance * 2

    total_duration = int(total_distance * 4)  # 15 km/h average

    return {
        "id": f"mock_{test_case['name'][:10]}",
        "segments": [
            {
                "coordinates": [(origin["lat"], origin["lng"])],
                "distance_km": total_distance,
                "elevation_gain_m": total_elevation,
                "elevation_loss_m": total_elevation * 0.9,
                "estimated_duration_min": total_duration,
                "surface_type": "paved",
            }
        ],
        "total_distance_km": round(total_distance, 1),
        "total_elevation_gain_m": round(total_elevation, 1),
        "total_duration_min": total_duration,
        "weather_forecasts": [
            {
                "time": "2025-03-15T07:00:00",
                "temperature": 15.0,
                "wind_speed": 3.5,
                "wind_direction": 180.0,
                "precipitation_probability": 20.0,
                "weather_code": 1,
                "description": "晴れ",
            },
            {
                "time": "2025-03-15T12:00:00",
                "temperature": 18.0,
                "wind_speed": 4.2,
                "wind_direction": 190.0,
                "precipitation_probability": 10.0,
                "weather_code": 2,
                "description": "やや曇り",
            },
        ],
        "llm_analysis": f"{origin.get('name')}から{destination.get('name')}への{difficulty}難易度ルート。"
        f"総距離約{total_distance:.1f}km、獲得標高約{total_elevation:.0f}m。"
        f"天候は概ね良好で、風も穏やか。補給は中間地点で確保を推奨。",
        "warnings": [
            "長距離ライドのため、十分な水分・補給食を携帯してください",
            "天候の急変に備え、レインウェアを携帯することを推奨します",
        ],
        "recommended_gear": [
            "ドリンク2本以上",
            "補給食（エネルギーバー3本以上）",
            "レインウェア",
            "予備チューブ・パンク修理キット",
            "携帯ポンプ",
        ],
        "created_at": datetime.now().isoformat(),
    }


async def run_evaluation() -> None:
    """Run evaluation on all test cases and save results."""
    print("=== Cycling Route Quality Evaluation ===\n")

    # Load test cases
    eval_dir = Path(__file__).parent
    test_routes_path = eval_dir / "test_routes.json"

    with open(test_routes_path, "r", encoding="utf-8") as f:
        test_cases = json.load(f)

    print(f"Loaded {len(test_cases)} test cases\n")

    # Initialize Anthropic client
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY environment variable not set")

    client = anthropic.AsyncAnthropic(api_key=api_key)

    # Evaluate each test case
    all_results = []

    for i, test_case in enumerate(test_cases, 1):
        print(f"[{i}/{len(test_cases)}] Evaluating: {test_case['name']}")

        try:
            # Try to import and use real planner
            # If not available, use mock data
            try:
                # This import will fail if backend/planner are not set up
                # from planner.route_planner import plan_route
                # route_plan = await plan_route(test_case)
                raise ImportError("Using mock for now")
            except ImportError:
                print("  - Using mock route plan (backend not available)")
                route_plan = await mock_plan_route(test_case)

            # Evaluate the route plan
            result = await evaluate_route_plan(test_case, route_plan, client)
            all_results.append(result)

            # Print scores
            evaluation = result["evaluation"]
            print(f"  - Safety: {evaluation['safety']['score']}/10")
            print(f"  - Weather: {evaluation['weather_integration']['score']}/10")
            print(f"  - Practicality: {evaluation['practicality']['score']}/10")
            print(f"  - User Satisfaction: {evaluation['user_satisfaction']['score']}/10")
            print(f"  - Overall: {evaluation['overall']['score']}/10")
            print()

        except Exception as e:
            print(f"  ERROR: {e}\n")
            all_results.append(
                {
                    "test_case_name": test_case["name"],
                    "error": str(e),
                    "evaluated_at": datetime.now().isoformat(),
                }
            )

    # Save results
    results_dir = eval_dir / "results"
    results_dir.mkdir(exist_ok=True)
    results_path = results_dir / "evaluation_results.json"

    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "evaluated_at": datetime.now().isoformat(),
                "total_cases": len(test_cases),
                "results": all_results,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    print(f"\n=== Evaluation Complete ===")
    print(f"Results saved to: {results_path}")

    # Print summary table
    print("\n=== Summary ===")
    print(f"{'Test Case':<50} {'Safety':<8} {'Weather':<8} {'Practical':<10} {'User Sat':<10} {'Overall':<8}")
    print("-" * 100)

    for result in all_results:
        if "error" in result:
            print(f"{result['test_case_name']:<50} ERROR: {result['error']}")
        else:
            ev = result["evaluation"]
            print(
                f"{result['test_case_name']:<50} "
                f"{ev['safety']['score']:<8} "
                f"{ev['weather_integration']['score']:<8} "
                f"{ev['practicality']['score']:<10} "
                f"{ev['user_satisfaction']['score']:<10} "
                f"{ev['overall']['score']:<8}"
            )


if __name__ == "__main__":
    asyncio.run(run_evaluation())
